{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ua8ztlP6M9U9",
    "outputId": "b70123d1-7b14-4226-8f97-7d175f518f4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done1\n"
     ]
    }
   ],
   "source": [
    "f= open('stop.txt', 'r') \n",
    "stop=f.read()\n",
    "f.close()\n",
    "print('Done1')\n",
    "stop=set(stop.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o64MIekNfcdb"
   },
   "outputs": [],
   "source": [
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkX9CaCi4d4e"
   },
   "outputs": [],
   "source": [
    "#Preprocessing for train data\n",
    "f= open('train.txt', 'r') \n",
    "contents=f.read()\n",
    "f.close()\n",
    "l=contents.split('\\n')\n",
    "a=[]\n",
    "for i in l:\n",
    "  a.append(i.split('\\t'))\n",
    "new=[]\n",
    "l=[]\n",
    "sen=[]\n",
    "s=''\n",
    "n=[]\n",
    "p=0\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "#removing emojis\n",
    "for i in range(len(a)):\n",
    "  a[i][0]=deEmojify(a[i][0])\n",
    "\n",
    "#removing @s  \n",
    "for i in a:\n",
    "  if('@' in i[0]):\n",
    "    p=2\n",
    "  else:  \n",
    "    if(p==2):\n",
    "      if(i[0]!='_'):\n",
    "        p=1\n",
    "      else:\n",
    "        p=2\n",
    "    elif(p==1):\n",
    "      if(i[0]!='_'):\n",
    "        p=0\n",
    "        n.append(i)\n",
    "      else:\n",
    "        p=2        \n",
    "    else:\n",
    "      n.append(i)    \n",
    "sent=[]\n",
    "\n",
    "#remove punctuations\n",
    "b=[]\n",
    "for i in range(len(n)):\n",
    "  s=''\n",
    "  for j in n[i][0]:\n",
    "    if(j not in punctuations):\n",
    "      s=s+j \n",
    "  n[i][0]=s\n",
    "  if(s!=''):\n",
    "    b.append(n[i]) \n",
    "n=b[:]\n",
    "b=[]\n",
    "for i in range(len(n)):\n",
    "  if(n[i][0]!='RT'):\n",
    "    b.append(n[i])\n",
    "n=b[:]    \n",
    "#sentence formation\n",
    "for i in n:\n",
    "  if(i[0]=='meta'):\n",
    "    new.append(l)\n",
    "    l=[]\n",
    "    sent.append(i[2])\n",
    "  else:  \n",
    "    l.append(i)\n",
    "new.append(l)\n",
    "new=new[1:]\n",
    "\n",
    "new2=[]\n",
    "#removing websites\n",
    "for i in new:\n",
    "  news=[]\n",
    "  for j in i:\n",
    "    if(j[0]=='https'):\n",
    "      break\n",
    "    else:\n",
    "      news.append(j)\n",
    "  new2.append(news) \n",
    "new3=[]\n",
    "for i in new2:\n",
    "  new=[]\n",
    "  for j in i:\n",
    "    if(j[0] not in stop):\n",
    "      new.append(j)\n",
    "      \n",
    "  new3.append(new)\n",
    "sen1=[]\n",
    "for i in new3:\n",
    "  s=''\n",
    "  for j in i:\n",
    "    s=s+j[0]+' '\n",
    "  sen1.append(s.strip())\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(list(map(list,zip(sen1,sent))),columns=['sen','emo'])\n",
    "df=df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0L26vPRxDFL"
   },
   "outputs": [],
   "source": [
    "#preprocessing for test data\n",
    "f= open('test.txt', 'r') \n",
    "contents=f.read()\n",
    "f.close()\n",
    "l=contents.split('\\n')\n",
    "a=[]\n",
    "for i in l:\n",
    "  a.append(i.split('\\t'))\n",
    "new=[]\n",
    "l=[]\n",
    "sen=[]\n",
    "s=''\n",
    "n=[]\n",
    "p=0\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "#removing emojis\n",
    "for i in range(len(a)):\n",
    "  a[i][0]=deEmojify(a[i][0])\n",
    "\n",
    "#removing @s  \n",
    "for i in a:\n",
    "  if('@' in i[0]):\n",
    "    p=2\n",
    "  else:  \n",
    "    if(p==2):\n",
    "      if(i[0]!='_'):\n",
    "        p=1\n",
    "      else:\n",
    "        p=2\n",
    "    elif(p==1):\n",
    "      if(i[0]!='_'):\n",
    "        p=0\n",
    "        n.append(i)\n",
    "      else:\n",
    "        p=2        \n",
    "    else:\n",
    "      n.append(i)    \n",
    "sent=[]\n",
    "\n",
    "#remove punctuations\n",
    "b=[]\n",
    "for i in range(len(n)):\n",
    "  s=''\n",
    "  for j in n[i][0]:\n",
    "    if(j not in punctuations):\n",
    "      s=s+j \n",
    "  n[i][0]=s\n",
    "  if(s!=''):\n",
    "    b.append(n[i]) \n",
    "n=b[:]\n",
    "b=[]\n",
    "for i in range(len(n)):\n",
    "  if(n[i][0]!='RT'):\n",
    "    b.append(n[i])\n",
    "n=b[:]    \n",
    "#sentence formation\n",
    "for i in n:\n",
    "  if(i[0]=='meta'):\n",
    "    new.append(l)\n",
    "    l=[]\n",
    "    sent.append(i[2])\n",
    "  else:  \n",
    "    l.append(i)\n",
    "new.append(l)\n",
    "new=new[1:]\n",
    "\n",
    "new2=[]\n",
    "#removing websites\n",
    "for i in new:\n",
    "  news=[]\n",
    "  for j in i:\n",
    "    if(j[0]=='https'):\n",
    "      break\n",
    "    else:\n",
    "      news.append(j)\n",
    "  new2.append(news) \n",
    "new3=[]\n",
    "for i in new2:\n",
    "  new=[]\n",
    "  for j in i:\n",
    "    if(j[0] not in stop):\n",
    "      new.append(j)\n",
    "      \n",
    "  new3.append(new)\n",
    "sen1=[]\n",
    "for i in new3:\n",
    "  s=''\n",
    "  for j in i:\n",
    "    s=s+j[0]+' '\n",
    "  sen1.append(s.strip())\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(list(map(list,zip(sen1,sent))),columns=['sen','emo'])\n",
    "df=df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "dGudbGTsfvQ1",
    "outputId": "f4a74b7c-d86c-4953-8484-9a8a8e51bd25"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11433</th>\n",
       "      <td>Paschim Bengal asansol v dholera smart city ba...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9316</th>\n",
       "      <td>Youre Smjhdaar xD haha alot</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>UPA dal dhiyan EVM sandeh savikar awor jeet EV...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13265</th>\n",
       "      <td>Tum logo bolne fark ni pdta usko Usk</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12491</th>\n",
       "      <td>Jali Rana ayyub kehtey Aur bujhi Thanks suppor...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9838</th>\n",
       "      <td>Tum kr Rahul Gandi chor jija Vadra gudgan rok</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>Tickets kharid advance bsdko line lag Police d...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>Yeah decision PAKISTAN zinadabad Pak army zind...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7654</th>\n",
       "      <td>Race3 FDSS Mai 2 movie show Mai Tubelight Race...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>On WomensDay SantRampalji maharaj supporting d...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15131 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     sen       emo\n",
       "11433  Paschim Bengal asansol v dholera smart city ba...   neutral\n",
       "9316                         Youre Smjhdaar xD haha alot  positive\n",
       "3567   UPA dal dhiyan EVM sandeh savikar awor jeet EV...   neutral\n",
       "13265               Tum logo bolne fark ni pdta usko Usk   neutral\n",
       "12491  Jali Rana ayyub kehtey Aur bujhi Thanks suppor...  positive\n",
       "...                                                  ...       ...\n",
       "9838       Tum kr Rahul Gandi chor jija Vadra gudgan rok   neutral\n",
       "10345  Tickets kharid advance bsdko line lag Police d...  negative\n",
       "2412   Yeah decision PAKISTAN zinadabad Pak army zind...  positive\n",
       "7654   Race3 FDSS Mai 2 movie show Mai Tubelight Race...  positive\n",
       "4947   On WomensDay SantRampalji maharaj supporting d...  positive\n",
       "\n",
       "[15131 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zu8ey-dBhGrA",
    "outputId": "64105349-5a30-4d71-ab5b-77edb896a64e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869"
      ]
     },
     "execution_count": 110,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "x0lDbTL4hrJ8",
    "outputId": "93afc6c5-8d4a-47e8-b018-4580758209b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f59473496a0>"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD7CAYAAAB5aaOHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#A view of the dataset\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "# titanic = sns.load_dataset(\"sent\")\n",
    "sns.countplot(x=sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vwrtuGuJDDH"
   },
   "outputs": [],
   "source": [
    "#Finding maxlen of a sentence from both test data and train data\n",
    "maxlen=0\n",
    "for i in (list(df['sen'])+list(df1['sen'])):\n",
    "  maxlen = max(maxlen,len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W92QcMTbTgVM"
   },
   "outputs": [],
   "source": [
    "#character level tokenisation\n",
    "import re\n",
    "def token(sentence, remove_vowels=False, remove_repeat=False, minchars=2):\n",
    "    tokens = []\n",
    "#   for t in re.findall(\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\w]+\",sentence.lower()):\n",
    "    for t in re.findall(\"[a-zA-Z]+\",sentence.lower()):\n",
    "\n",
    "        if len(t)>=minchars:\n",
    "            if remove_vowels:\n",
    "                t=removeVovels(t)\n",
    "            if remove_repeat:\n",
    "                t=removeRepeat(t)\n",
    "            tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "VOWELS = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "def removeRepeat(string):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', string)     \n",
    "\n",
    "def removeVovels(string):\n",
    "    return ''.join([l for l in string.lower() if l not in VOWELS])\n",
    "\n",
    "def normalize_matrix(matrix):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzSnHu9eUUkq"
   },
   "outputs": [],
   "source": [
    "#X_train and Y_train\n",
    "import numpy as np\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "#Processes individual lines\n",
    "for line in list(df['sen']):\n",
    "  # Seperator for the current dataset. Currently '\\t'. \n",
    "  #Token is the function which implements basic preprocessing as mentioned in our paper\n",
    "  tokenized_lines = token(line)\n",
    "  \n",
    "  #Creates character lists\n",
    "  char_list = []\n",
    "  for words in tokenized_lines:\n",
    "    for char in words:\n",
    "      char_list.append(char)\n",
    "    char_list.append(' ')\n",
    "  #print(char_list) - Debugs the character list created\n",
    "  X_train.append(char_list)\n",
    "  \n",
    "  #Appends labels\n",
    "for line in list(df['emo']):  \n",
    "  if line == 'neutral':\n",
    "    Y_train.append(0)\n",
    "  if line == 'positive':\n",
    "    Y_train.append(1)\n",
    "  if line == 'negative':\n",
    "    Y_train.append(2)\n",
    "\n",
    "#Converts Y_train to a numpy array\t\n",
    "Y_train = np.asarray(Y_train)\n",
    "assert(len(X_train) == Y_train.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u3pdqK5IG4Vp",
    "outputId": "30ea78d6-2277-47f7-d102-5a81ffb2529e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15131"
      ]
     },
     "execution_count": 103,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlVTTIfyxm-R"
   },
   "outputs": [],
   "source": [
    "#X_test and Y_test\n",
    "import numpy as np\n",
    "\n",
    "X_test = []\n",
    "Y_test= []\n",
    "\n",
    "#Processes individual lines\n",
    "for line in list(df1['sen']):\n",
    "  # Seperator for the current dataset. Currently '\\t'. \n",
    "  #Token is the function which implements basic preprocessing as mentioned in our paper\n",
    "  tokenized_lines = token(line)\n",
    "  \n",
    "  #Creates character lists\n",
    "  char_list = []\n",
    "  for words in tokenized_lines:\n",
    "    for char in words:\n",
    "      char_list.append(char)\n",
    "    char_list.append(' ')\n",
    "  #print(char_list) - Debugs the character list created\n",
    "  X_test.append(char_list)\n",
    "  \n",
    "  #Appends labels\n",
    "for line in list(df1['emo']):  \n",
    "  if line == 'neutral':\n",
    "    Y_test.append(0)\n",
    "  if line == 'positive':\n",
    "    Y_test.append(1)\n",
    "  if line == 'negative':\n",
    "    Y_test.append(2)\n",
    "\n",
    "#Converts Y_train to a numpy array\t\n",
    "Y_test = np.asarray(Y_test)\n",
    "assert(len(X_test) == Y_test.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "QmZeHls3wzEL",
    "outputId": "b875f747-0115-4048-c62d-d1b1d87844d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 0, 'x': 1, 'z': 2, 's': 3, 'l': 4, 'm': 5, 'w': 6, 'a': 7, 'd': 8, 'r': 9, 'c': 10, 'e': 11, 'b': 12, 'p': 13, 'q': 14, 'j': 15, 'o': 16, 'v': 17, 'g': 18, ' ': 19, 'k': 20, 'n': 21, 'i': 22, 'f': 23, 'h': 24, 'u': 25, 'y': 26}\n",
      "{0: 't', 1: 'x', 2: 'z', 3: 's', 4: 'l', 5: 'm', 6: 'w', 7: 'a', 8: 'd', 9: 'r', 10: 'c', 11: 'e', 12: 'b', 13: 'p', 14: 'q', 15: 'j', 16: 'o', 17: 'v', 18: 'g', 19: ' ', 20: 'k', 21: 'n', 22: 'i', 23: 'f', 24: 'h', 25: 'u', 26: 'y'}\n"
     ]
    }
   ],
   "source": [
    "#Creating the dictionaries for encoding the sentences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "char2num=dict()\n",
    "num2char=dict()\n",
    "\"\"\"\n",
    "Purpose -> Convert characters to integers, a unique value for every character\n",
    "Input   -> Training data (In list of lists format) along with global variables\n",
    "Output  -> Converted training data along with global variables\n",
    "\"\"\"\n",
    "allchars = []\n",
    "errors = 0\n",
    "\n",
    "#Creates a list of all characters present in the dataset\n",
    "for line in X_train+X_test:\n",
    "\ttry:\n",
    "\t\tallchars = set(allchars+line)\n",
    "\t\tallchars = list(allchars)\n",
    "\texcept:\n",
    "\t\terrors += 1\n",
    "\n",
    "#print(errors) #Debugging\n",
    "#print(allchars) #Debugging \n",
    "\n",
    "#Creates character dictionaries for the characters\n",
    "charno = 0\n",
    "for char in allchars:\n",
    "\tchar2num[char] = charno\n",
    "\tnum2char[charno] = char\n",
    "\tcharno += 1\n",
    "\n",
    "assert(len(allchars)==charno) #Checks\n",
    "\n",
    "print(char2num)\n",
    "print(num2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vgmekgyuWVUf"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def encode(trainwords,maxlen):\n",
    "\n",
    "\tlines = []\n",
    "\tfor line in trainwords:\n",
    "\t\tchar_list=[]\n",
    "\t\tfor letter in line:\n",
    "\t\t\tchar_list.append(char2num[letter])\n",
    "\t\t#print(no) -- Debugs the number mappings\n",
    "\t\tlines.append(char_list)\n",
    "\t#Pads the X_train to get a uniform vector\n",
    "\t#TODO: Automate the selection instead of manual input\n",
    "\tlines = pad_sequences(lines[:], maxlen=maxlen)\n",
    "\treturn lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uswXmnnpypLj"
   },
   "outputs": [],
   "source": [
    "#The final encoded data\n",
    "out=encode(X_test,maxlen)\n",
    "X_test=np.asarray(out)\n",
    "y_test = np.asarray(Y_test).flatten()\n",
    "y_test = to_categorical(y_test, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lybPmnbrXiUr"
   },
   "outputs": [],
   "source": [
    "#The final encoded data\n",
    "out=encode(X_train,maxlen)\n",
    "X_train = np.asarray(out)\n",
    "y_train = np.asarray(Y_train).flatten()\n",
    "y_train = to_categorical(y_train, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZIyyda1QydAb",
    "outputId": "356a958b-ca20-4cd6-d56a-a1930af5d81a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15131, 135), (1869, 135), (15131, 3), (1869, 3))"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train),np.shape(X_test),np.shape(y_train),np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWF7t8V847tk"
   },
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Tf_gfdfx0B_M",
    "outputId": "1845a56a-f7d9-4bec-a861-19dfbf66c35e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13617, 135)"
      ]
     },
     "execution_count": 113,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras import Sequential\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910
    },
    "colab_type": "code",
    "id": "t-kvRxsAZzaV",
    "outputId": "d0151eb4-cb02-4be4-d44d-a150bdeb1e22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/24\n",
      "13617/13617 [==============================] - 21s 2ms/step - loss: 1.0594 - acc: 0.4198 - f1_m: 0.1168 - precision_m: 0.4989 - recall_m: 0.0695 - val_loss: 1.0241 - val_acc: 0.4663 - val_f1_m: 0.2597 - val_precision_m: 0.6085 - val_recall_m: 0.1664\n",
      "Epoch 2/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 1.0043 - acc: 0.4866 - f1_m: 0.2957 - precision_m: 0.5952 - recall_m: 0.1984 - val_loss: 0.9876 - val_acc: 0.5033 - val_f1_m: 0.3416 - val_precision_m: 0.6050 - val_recall_m: 0.2391\n",
      "Epoch 3/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.9775 - acc: 0.5098 - f1_m: 0.3384 - precision_m: 0.6122 - recall_m: 0.2364 - val_loss: 0.9664 - val_acc: 0.5198 - val_f1_m: 0.3611 - val_precision_m: 0.6203 - val_recall_m: 0.2556\n",
      "Epoch 4/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.9499 - acc: 0.5320 - f1_m: 0.3959 - precision_m: 0.6327 - recall_m: 0.2901 - val_loss: 0.9506 - val_acc: 0.5383 - val_f1_m: 0.4011 - val_precision_m: 0.6301 - val_recall_m: 0.2952\n",
      "Epoch 5/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.9294 - acc: 0.5459 - f1_m: 0.4275 - precision_m: 0.6334 - recall_m: 0.3239 - val_loss: 0.9622 - val_acc: 0.5284 - val_f1_m: 0.4401 - val_precision_m: 0.5997 - val_recall_m: 0.3481\n",
      "Epoch 6/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.9254 - acc: 0.5549 - f1_m: 0.4410 - precision_m: 0.6387 - recall_m: 0.3390 - val_loss: 0.9411 - val_acc: 0.5350 - val_f1_m: 0.4084 - val_precision_m: 0.6065 - val_recall_m: 0.3085\n",
      "Epoch 7/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.9101 - acc: 0.5618 - f1_m: 0.4516 - precision_m: 0.6495 - recall_m: 0.3484 - val_loss: 0.9306 - val_acc: 0.5436 - val_f1_m: 0.4427 - val_precision_m: 0.6171 - val_recall_m: 0.3461\n",
      "Epoch 8/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8983 - acc: 0.5679 - f1_m: 0.4818 - precision_m: 0.6484 - recall_m: 0.3852 - val_loss: 0.9398 - val_acc: 0.5244 - val_f1_m: 0.4353 - val_precision_m: 0.6180 - val_recall_m: 0.3369\n",
      "Epoch 9/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8953 - acc: 0.5711 - f1_m: 0.4810 - precision_m: 0.6469 - recall_m: 0.3846 - val_loss: 0.9153 - val_acc: 0.5555 - val_f1_m: 0.4609 - val_precision_m: 0.6221 - val_recall_m: 0.3666\n",
      "Epoch 10/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8765 - acc: 0.5831 - f1_m: 0.5098 - precision_m: 0.6584 - recall_m: 0.4174 - val_loss: 0.9167 - val_acc: 0.5535 - val_f1_m: 0.4794 - val_precision_m: 0.6050 - val_recall_m: 0.3976\n",
      "Epoch 11/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8745 - acc: 0.5849 - f1_m: 0.5153 - precision_m: 0.6524 - recall_m: 0.4275 - val_loss: 0.9069 - val_acc: 0.5522 - val_f1_m: 0.4815 - val_precision_m: 0.6159 - val_recall_m: 0.3956\n",
      "Epoch 12/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8666 - acc: 0.5920 - f1_m: 0.5262 - precision_m: 0.6571 - recall_m: 0.4397 - val_loss: 0.8941 - val_acc: 0.5654 - val_f1_m: 0.4857 - val_precision_m: 0.6463 - val_recall_m: 0.3897\n",
      "Epoch 13/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8581 - acc: 0.5953 - f1_m: 0.5370 - precision_m: 0.6609 - recall_m: 0.4533 - val_loss: 0.8969 - val_acc: 0.5694 - val_f1_m: 0.5065 - val_precision_m: 0.6310 - val_recall_m: 0.4234\n",
      "Epoch 14/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8559 - acc: 0.5989 - f1_m: 0.5414 - precision_m: 0.6668 - recall_m: 0.4574 - val_loss: 0.8967 - val_acc: 0.5647 - val_f1_m: 0.5001 - val_precision_m: 0.6384 - val_recall_m: 0.4115\n",
      "Epoch 15/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8516 - acc: 0.5972 - f1_m: 0.5435 - precision_m: 0.6568 - recall_m: 0.4646 - val_loss: 0.9199 - val_acc: 0.5588 - val_f1_m: 0.4917 - val_precision_m: 0.6101 - val_recall_m: 0.4128\n",
      "Epoch 16/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8451 - acc: 0.6077 - f1_m: 0.5517 - precision_m: 0.6695 - recall_m: 0.4708 - val_loss: 0.8888 - val_acc: 0.5687 - val_f1_m: 0.4998 - val_precision_m: 0.6491 - val_recall_m: 0.4069\n",
      "Epoch 17/24\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.8413 - acc: 0.6069 - f1_m: 0.5582 - precision_m: 0.6670 - recall_m: 0.4806 - val_loss: 0.8947 - val_acc: 0.5740 - val_f1_m: 0.5205 - val_precision_m: 0.6238 - val_recall_m: 0.4472\n",
      "Epoch 18/24\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.8330 - acc: 0.6129 - f1_m: 0.5638 - precision_m: 0.6644 - recall_m: 0.4905 - val_loss: 0.8872 - val_acc: 0.5753 - val_f1_m: 0.5121 - val_precision_m: 0.6466 - val_recall_m: 0.4247\n",
      "Epoch 19/24\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.8353 - acc: 0.6114 - f1_m: 0.5593 - precision_m: 0.6690 - recall_m: 0.4818 - val_loss: 0.8840 - val_acc: 0.5700 - val_f1_m: 0.5054 - val_precision_m: 0.6543 - val_recall_m: 0.4122\n",
      "Epoch 20/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8316 - acc: 0.6122 - f1_m: 0.5659 - precision_m: 0.6701 - recall_m: 0.4909 - val_loss: 0.8918 - val_acc: 0.5760 - val_f1_m: 0.5323 - val_precision_m: 0.6244 - val_recall_m: 0.4643\n",
      "Epoch 21/24\n",
      "13617/13617 [==============================] - 16s 1ms/step - loss: 0.8239 - acc: 0.6185 - f1_m: 0.5761 - precision_m: 0.6745 - recall_m: 0.5036 - val_loss: 0.8856 - val_acc: 0.5760 - val_f1_m: 0.5178 - val_precision_m: 0.6284 - val_recall_m: 0.4412\n",
      "Epoch 22/24\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.8176 - acc: 0.6236 - f1_m: 0.5824 - precision_m: 0.6744 - recall_m: 0.5133 - val_loss: 0.8798 - val_acc: 0.5885 - val_f1_m: 0.5327 - val_precision_m: 0.6275 - val_recall_m: 0.4637\n",
      "Epoch 23/24\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.8165 - acc: 0.6224 - f1_m: 0.5833 - precision_m: 0.6789 - recall_m: 0.5123 - val_loss: 0.8825 - val_acc: 0.5760 - val_f1_m: 0.5183 - val_precision_m: 0.6264 - val_recall_m: 0.4425\n",
      "Epoch 24/24\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.8089 - acc: 0.6277 - f1_m: 0.5904 - precision_m: 0.6780 - recall_m: 0.5238 - val_loss: 0.8826 - val_acc: 0.5786 - val_f1_m: 0.5287 - val_precision_m: 0.6355 - val_recall_m: 0.4531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0771a7cfd0>"
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sub-word level Model\n",
    "model = Sequential()\n",
    "\n",
    "#Sub words embedding generated through the convolution and maxpooling\n",
    "\n",
    "model.add(Embedding(charno, 128, input_length=maxlen))\n",
    "model.add(Conv1D(128, 3,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "#sub word level embedding generated from character level\n",
    "\n",
    "#start of prediction\n",
    "\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Optimizer is Adamax along with categorical crossentropy loss\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='adamax',\n",
    "        metrics=['accuracy',f1_m,precision_m,recall_m])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "      batch_size=128, \n",
    "      shuffle=True, \n",
    "      epochs=24,\n",
    "      validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Be8Cv7HnGbEi",
    "outputId": "63bc32fc-a55c-4589-9065-e21f5d201737",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.8056 - acc: 0.6295 - f1_m: 0.5946 - precision_m: 0.6802 - recall_m: 0.5289 - val_loss: 0.8843 - val_acc: 0.5760 - val_f1_m: 0.5407 - val_precision_m: 0.6340 - val_recall_m: 0.4723\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9809805275349135, 0.543606206718902, 0.49246345987416895, 0.5876373881449223, 0.42589620119304566]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.8071 - acc: 0.6294 - f1_m: 0.5937 - precision_m: 0.6801 - recall_m: 0.5274 - val_loss: 0.8985 - val_acc: 0.5720 - val_f1_m: 0.5263 - val_precision_m: 0.6230 - val_recall_m: 0.4564\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0036017878161452, 0.5222043875642739, 0.4776989785728128, 0.5687397216462144, 0.41359015517913444]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.8014 - acc: 0.6315 - f1_m: 0.5980 - precision_m: 0.6801 - recall_m: 0.5345 - val_loss: 0.8813 - val_acc: 0.5832 - val_f1_m: 0.5330 - val_precision_m: 0.6276 - val_recall_m: 0.4637\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9786137759589842, 0.5387907973772195, 0.48860334520865534, 0.5854848839816976, 0.4210807920267647]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7982 - acc: 0.6349 - f1_m: 0.6018 - precision_m: 0.6881 - recall_m: 0.5357 - val_loss: 0.8900 - val_acc: 0.5760 - val_f1_m: 0.5561 - val_precision_m: 0.6189 - val_recall_m: 0.5053\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9946757667902767, 0.5414660246280376, 0.5056794426733316, 0.5750174118019541, 0.4531835206470877]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7921 - acc: 0.6371 - f1_m: 0.6046 - precision_m: 0.6848 - recall_m: 0.5420 - val_loss: 0.9053 - val_acc: 0.5766 - val_f1_m: 0.5348 - val_precision_m: 0.6091 - val_recall_m: 0.4775\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.00329401436944, 0.5189941146910797, 0.4864226881037937, 0.5531572792988018, 0.4355270198126283]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7933 - acc: 0.6380 - f1_m: 0.6055 - precision_m: 0.6878 - recall_m: 0.5415 - val_loss: 0.8934 - val_acc: 0.5793 - val_f1_m: 0.5516 - val_precision_m: 0.6231 - val_recall_m: 0.4954\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9968422336843704, 0.5195291601699453, 0.48626610635116563, 0.5579275716875504, 0.43338683807256706]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7894 - acc: 0.6391 - f1_m: 0.6097 - precision_m: 0.6854 - recall_m: 0.5500 - val_loss: 0.8800 - val_acc: 0.5931 - val_f1_m: 0.5528 - val_precision_m: 0.6350 - val_recall_m: 0.4901\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9884299771105816, 0.5446762976766334, 0.504760222042098, 0.5866672675707814, 0.44515783843221096]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7889 - acc: 0.6411 - f1_m: 0.6066 - precision_m: 0.6877 - recall_m: 0.5434 - val_loss: 0.8846 - val_acc: 0.5878 - val_f1_m: 0.5497 - val_precision_m: 0.6408 - val_recall_m: 0.4815\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.002025952818816, 0.5243445694797367, 0.48427105638801, 0.5719440088289954, 0.42215088284098573]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7864 - acc: 0.6443 - f1_m: 0.6149 - precision_m: 0.6889 - recall_m: 0.5558 - val_loss: 0.8840 - val_acc: 0.5766 - val_f1_m: 0.5420 - val_precision_m: 0.6255 - val_recall_m: 0.4789\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9858315346584147, 0.5318352061838565, 0.4825439459184327, 0.5674477133794288, 0.42161583736212005]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7775 - acc: 0.6477 - f1_m: 0.6221 - precision_m: 0.6978 - recall_m: 0.5618 - val_loss: 0.8892 - val_acc: 0.5799 - val_f1_m: 0.5572 - val_precision_m: 0.6241 - val_recall_m: 0.5040\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0086908137626096, 0.5420010701069033, 0.5053532694736989, 0.5715037004522872, 0.45478865705179367]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7808 - acc: 0.6449 - f1_m: 0.6158 - precision_m: 0.6899 - recall_m: 0.5567 - val_loss: 0.9007 - val_acc: 0.5826 - val_f1_m: 0.5402 - val_precision_m: 0.6253 - val_recall_m: 0.4762\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0136529432062544, 0.527019796842174, 0.4851781828254222, 0.5701869369956374, 0.4242910649318501]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7867 - acc: 0.6418 - f1_m: 0.6142 - precision_m: 0.6873 - recall_m: 0.5558 - val_loss: 0.8896 - val_acc: 0.5793 - val_f1_m: 0.5362 - val_precision_m: 0.6320 - val_recall_m: 0.4663\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.972175812070672, 0.5377207064194881, 0.48363240999994334, 0.5796999010955541, 0.4173354735311944]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7750 - acc: 0.6463 - f1_m: 0.6185 - precision_m: 0.6936 - recall_m: 0.5589 - val_loss: 0.8915 - val_acc: 0.5845 - val_f1_m: 0.5537 - val_precision_m: 0.6147 - val_recall_m: 0.5040\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0027306769989976, 0.5420010701069033, 0.5085037800320461, 0.5711005324664251, 0.4601391118404507]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7721 - acc: 0.6467 - f1_m: 0.6235 - precision_m: 0.6968 - recall_m: 0.5650 - val_loss: 0.8872 - val_acc: 0.5832 - val_f1_m: 0.5452 - val_precision_m: 0.6418 - val_recall_m: 0.4742\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9862196233388636, 0.5307651152261251, 0.4849988657755926, 0.5781258094304382, 0.4200107009255229]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7688 - acc: 0.6526 - f1_m: 0.6267 - precision_m: 0.6986 - recall_m: 0.5688 - val_loss: 0.8999 - val_acc: 0.5865 - val_f1_m: 0.5561 - val_precision_m: 0.6151 - val_recall_m: 0.5079\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0139474498836545, 0.5409309793245735, 0.5093439604714314, 0.5747802223095351, 0.4590690208827193]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7637 - acc: 0.6516 - f1_m: 0.6264 - precision_m: 0.6972 - recall_m: 0.5691 - val_loss: 0.8882 - val_acc: 0.5826 - val_f1_m: 0.5518 - val_precision_m: 0.6377 - val_recall_m: 0.4868\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9995483659435046, 0.5403959338138166, 0.48906366629929643, 0.5755183852694773, 0.42696629229428745]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7591 - acc: 0.6576 - f1_m: 0.6300 - precision_m: 0.6980 - recall_m: 0.5747 - val_loss: 0.8893 - val_acc: 0.5786 - val_f1_m: 0.5507 - val_precision_m: 0.6266 - val_recall_m: 0.4914\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9942840885644039, 0.5371856609725135, 0.49208567574931183, 0.5699950025131385, 0.4349919745091642]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7679 - acc: 0.6557 - f1_m: 0.6282 - precision_m: 0.6980 - recall_m: 0.5715 - val_loss: 0.8923 - val_acc: 0.5806 - val_f1_m: 0.5505 - val_precision_m: 0.6200 - val_recall_m: 0.4954\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[0.9987520899856837, 0.541466024771548, 0.49519157435180927, 0.5674573890725325, 0.4408774747447957]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7575 - acc: 0.6609 - f1_m: 0.6353 - precision_m: 0.7024 - recall_m: 0.5802 - val_loss: 0.9075 - val_acc: 0.5740 - val_f1_m: 0.5492 - val_precision_m: 0.6086 - val_recall_m: 0.5007\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0059216338533206, 0.5323702516627222, 0.500809306445257, 0.5615965113593908, 0.4531835206151965]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7588 - acc: 0.6595 - f1_m: 0.6368 - precision_m: 0.7018 - recall_m: 0.5834 - val_loss: 0.8925 - val_acc: 0.5872 - val_f1_m: 0.5585 - val_precision_m: 0.6259 - val_recall_m: 0.5046\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0026445441095506, 0.5425361157292794, 0.5041540749510576, 0.5713773191198428, 0.4526484752798412]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7507 - acc: 0.6655 - f1_m: 0.6418 - precision_m: 0.7095 - recall_m: 0.5868 - val_loss: 0.9125 - val_acc: 0.5773 - val_f1_m: 0.5599 - val_precision_m: 0.6156 - val_recall_m: 0.5139\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.030254120269263, 0.5500267524333993, 0.5089060462943495, 0.5689624028450796, 0.4617442484524494]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7459 - acc: 0.6686 - f1_m: 0.6444 - precision_m: 0.7097 - recall_m: 0.5907 - val_loss: 0.9073 - val_acc: 0.5931 - val_f1_m: 0.5614 - val_precision_m: 0.6131 - val_recall_m: 0.5178\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0284091183500152, 0.5339753880993193, 0.5105464133494292, 0.564616616809349, 0.4670947032411064]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7496 - acc: 0.6651 - f1_m: 0.6444 - precision_m: 0.7101 - recall_m: 0.5903 - val_loss: 0.9176 - val_acc: 0.5898 - val_f1_m: 0.5534 - val_precision_m: 0.6151 - val_recall_m: 0.5033\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0278831233373615, 0.5436062066551196, 0.5157302191641446, 0.5784493816479315, 0.4665596577622407]\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/1\n",
      "13617/13617 [==============================] - 15s 1ms/step - loss: 0.7470 - acc: 0.6656 - f1_m: 0.6402 - precision_m: 0.7063 - recall_m: 0.5859 - val_loss: 0.8970 - val_acc: 0.5859 - val_f1_m: 0.5671 - val_precision_m: 0.6282 - val_recall_m: 0.5172\n",
      "1869/1869 [==============================] - 3s 2ms/step\n",
      "[1.0028393304622256, 0.543606206718902, 0.5136723921476296, 0.5781007611923769, 0.46388443036791216]\n"
     ]
    }
   ],
   "source": [
    "#To try to increase test f1 score.\n",
    "f1=[]\n",
    "for i in range(24):\n",
    "  model.fit(X_train, y_train, \n",
    "      batch_size=128, \n",
    "      shuffle=True, \n",
    "      epochs=1,\n",
    "      validation_data=(X_val, y_val))\n",
    "  f1.append(model.evaluate(X_test,y_test))\n",
    "  print(f1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start of Char CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "XylqSzSHzqRs",
    "outputId": "d202b289-f396-4de1-89fb-821ff6dc0a39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p': 1, 'a': 2, 's': 3, 'c': 4, 'h': 5, 'i': 6, 'm': 7, ' ': 8, 'b': 9, 'e': 10, 'n': 11, 'g': 12, 'l': 13, 'o': 14, 'v': 15, 'd': 16, 'r': 17, 't': 18, 'y': 19, 'j': 20, 'k': 21, 'u': 22, 'x': 23, 'w': 24, 'f': 25, '2': 26, '0': 27, '4': 28, '3': 29, 'q': 30, 'z': 31, '9': 32, '8': 33, '7': 34, '6': 35, '1': 36, '5': 37, '+': 38, '|': 39, '=': 40, '`': 41, 'UNK': 42}\n",
      "(43, 42)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generation of X_train, X_test, y_train and y_test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts=list(df['sen'])\n",
    "test_texts=list(df1['sen'])\n",
    "y_train=list(df['emo'])\n",
    "y_test=list(df1['emo'])\n",
    "train_texts = [s.lower() for s in train_texts]\n",
    "test_texts = [s.lower() for s in test_texts]\n",
    "# t5=[s.lower() for s in t5]\n",
    "#=======================Convert string to index================\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "tk.word_index = char_dict.copy()\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "\n",
    "train_sequences = tk.texts_to_sequences(train_texts)\n",
    "test_texts = tk.texts_to_sequences(test_texts)\n",
    "# t5=tk.texts_to_sequences(t5)\n",
    "train_data = pad_sequences(train_sequences, maxlen=maxlen, padding='post')\n",
    "test_data = pad_sequences(test_texts, maxlen=maxlen, padding='post')\n",
    "# t5=pad_sequences(t5, maxlen=maxlen, padding='post')\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "test_data = np.array(test_data, dtype='float32')\n",
    "# t5 = np.array(t5, dtype='float32')\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([{'negative', 'positive', 'neutral'}])\n",
    "# y_train=mlb.fit_transform(y_train)\n",
    "# y_test=mlb.fit_transform(y_test)\n",
    "\n",
    "train_classes = np.array(y_train)\n",
    "test_classes=np.array(y_test)\n",
    "X_train = train_data[:]\n",
    "y_train = train_classes[:]\n",
    "X_test = test_data[:]\n",
    "y_test = test_classes[:]\n",
    "\n",
    "# y=to_categorical(h2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)\n",
    "\n",
    "#encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X = [['negative'], ['positive'], ['neutral']]\n",
    "enc.fit(X)\n",
    "y_train=enc.transform(y_train.reshape(-1,1))\n",
    "y_test=enc.transform(y_test.reshape(-1,1))\n",
    "y_val=enc.transform(y_val.reshape(-1,1))\n",
    "\n",
    "#generate character level embeddings\n",
    "print(tk.word_index)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "embedding_weights = [] #(71, 70)\n",
    "embedding_weights.append(np.zeros(vocab_size)) # first row is pad\n",
    "\n",
    "for char, i in tk.word_index.items(): # from index 1 to 70\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print(embedding_weights.shape) # first row all 0 for PAD, 69 char, last row for UNK\n",
    "embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras import Sequential\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwlrLPNs4fb4"
   },
   "outputs": [],
   "source": [
    "# parameter\n",
    "input_size = maxlen\n",
    "# vocab_size = 69\n",
    "embedding_size = embedding_weights.shape[1]\n",
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 7, 3],\n",
    "               [256, 3, -1]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 3\n",
    "dropout_p = 0.3\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zcdTMOJKT9bp",
    "outputId": "3517f8ba-e0d5-4fb2-d1a0-133610120599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 135)               0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 135, 42)           1806      \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 129, 256)          75520     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 129, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 43, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 37, 256)           459008    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 37, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 12, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 10, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              2622464   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 4,408,337\n",
      "Trainable params: 4,408,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13617 samples, validate on 1514 samples\n",
      "Epoch 1/25\n",
      " - 6s - loss: 1.0698 - acc: 0.4049 - f1_m: 0.0817 - precision_m: 0.3277 - recall_m: 0.0506 - val_loss: 0.9964 - val_acc: 0.4848 - val_f1_m: 0.2979 - val_precision_m: 0.5821 - val_recall_m: 0.2015\n",
      "Epoch 2/25\n",
      " - 1s - loss: 0.9398 - acc: 0.5263 - f1_m: 0.4101 - precision_m: 0.6235 - recall_m: 0.3134 - val_loss: 0.9505 - val_acc: 0.5172 - val_f1_m: 0.4533 - val_precision_m: 0.5989 - val_recall_m: 0.3653\n",
      "Epoch 3/25\n",
      " - 1s - loss: 0.8510 - acc: 0.5988 - f1_m: 0.5283 - precision_m: 0.6624 - recall_m: 0.4425 - val_loss: 0.8857 - val_acc: 0.5773 - val_f1_m: 0.5128 - val_precision_m: 0.6316 - val_recall_m: 0.4320\n",
      "Epoch 4/25\n",
      " - 1s - loss: 0.7496 - acc: 0.6618 - f1_m: 0.6370 - precision_m: 0.6933 - recall_m: 0.5907 - val_loss: 0.9211 - val_acc: 0.5548 - val_f1_m: 0.5335 - val_precision_m: 0.5995 - val_recall_m: 0.4808\n",
      "Epoch 5/25\n",
      " - 1s - loss: 0.6350 - acc: 0.7203 - f1_m: 0.7112 - precision_m: 0.7350 - recall_m: 0.6897 - val_loss: 1.0772 - val_acc: 0.5119 - val_f1_m: 0.5026 - val_precision_m: 0.5166 - val_recall_m: 0.4894\n",
      "Epoch 6/25\n",
      " - 1s - loss: 0.4827 - acc: 0.8044 - f1_m: 0.8032 - precision_m: 0.8093 - recall_m: 0.7972 - val_loss: 1.1924 - val_acc: 0.5436 - val_f1_m: 0.5395 - val_precision_m: 0.5504 - val_recall_m: 0.5291\n",
      "Epoch 7/25\n",
      " - 1s - loss: 0.3614 - acc: 0.8646 - f1_m: 0.8643 - precision_m: 0.8665 - recall_m: 0.8621 - val_loss: 1.3989 - val_acc: 0.5396 - val_f1_m: 0.5379 - val_precision_m: 0.5409 - val_recall_m: 0.5350\n",
      "Epoch 8/25\n",
      " - 1s - loss: 0.2796 - acc: 0.9002 - f1_m: 0.8998 - precision_m: 0.9010 - recall_m: 0.8986 - val_loss: 1.5171 - val_acc: 0.5476 - val_f1_m: 0.5477 - val_precision_m: 0.5491 - val_recall_m: 0.5462\n",
      "Epoch 9/25\n",
      " - 1s - loss: 0.2116 - acc: 0.9288 - f1_m: 0.9286 - precision_m: 0.9294 - recall_m: 0.9277 - val_loss: 1.8110 - val_acc: 0.5449 - val_f1_m: 0.5448 - val_precision_m: 0.5453 - val_recall_m: 0.5443\n",
      "Epoch 10/25\n",
      " - 1s - loss: 0.1765 - acc: 0.9401 - f1_m: 0.9403 - precision_m: 0.9412 - recall_m: 0.9393 - val_loss: 1.7011 - val_acc: 0.5509 - val_f1_m: 0.5486 - val_precision_m: 0.5511 - val_recall_m: 0.5462\n",
      "Epoch 11/25\n",
      " - 1s - loss: 0.1564 - acc: 0.9493 - f1_m: 0.9495 - precision_m: 0.9505 - recall_m: 0.9484 - val_loss: 1.7375 - val_acc: 0.5694 - val_f1_m: 0.5681 - val_precision_m: 0.5688 - val_recall_m: 0.5674\n",
      "Epoch 12/25\n",
      " - 1s - loss: 0.1343 - acc: 0.9556 - f1_m: 0.9559 - precision_m: 0.9567 - recall_m: 0.9552 - val_loss: 1.9454 - val_acc: 0.5423 - val_f1_m: 0.5420 - val_precision_m: 0.5437 - val_recall_m: 0.5403\n",
      "Epoch 13/25\n",
      " - 1s - loss: 0.1400 - acc: 0.9506 - f1_m: 0.9513 - precision_m: 0.9534 - recall_m: 0.9492 - val_loss: 1.8903 - val_acc: 0.5495 - val_f1_m: 0.5491 - val_precision_m: 0.5507 - val_recall_m: 0.5476\n",
      "Epoch 14/25\n",
      " - 1s - loss: 0.1275 - acc: 0.9581 - f1_m: 0.9581 - precision_m: 0.9594 - recall_m: 0.9567 - val_loss: 2.1712 - val_acc: 0.5225 - val_f1_m: 0.5213 - val_precision_m: 0.5227 - val_recall_m: 0.5198\n",
      "Epoch 15/25\n",
      " - 1s - loss: 0.0902 - acc: 0.9693 - f1_m: 0.9692 - precision_m: 0.9704 - recall_m: 0.9680 - val_loss: 2.2157 - val_acc: 0.5509 - val_f1_m: 0.5491 - val_precision_m: 0.5521 - val_recall_m: 0.5462\n",
      "Epoch 16/25\n",
      " - 1s - loss: 0.0962 - acc: 0.9663 - f1_m: 0.9664 - precision_m: 0.9685 - recall_m: 0.9644 - val_loss: 2.3082 - val_acc: 0.5542 - val_f1_m: 0.5526 - val_precision_m: 0.5543 - val_recall_m: 0.5509\n",
      "Epoch 17/25\n",
      " - 1s - loss: 0.0839 - acc: 0.9703 - f1_m: 0.9706 - precision_m: 0.9729 - recall_m: 0.9684 - val_loss: 2.2017 - val_acc: 0.5482 - val_f1_m: 0.5477 - val_precision_m: 0.5505 - val_recall_m: 0.5449\n",
      "Epoch 18/25\n",
      " - 1s - loss: 0.0814 - acc: 0.9697 - f1_m: 0.9699 - precision_m: 0.9721 - recall_m: 0.9678 - val_loss: 2.3256 - val_acc: 0.5310 - val_f1_m: 0.5313 - val_precision_m: 0.5329 - val_recall_m: 0.5297\n",
      "Epoch 19/25\n",
      " - 1s - loss: 0.0787 - acc: 0.9709 - f1_m: 0.9713 - precision_m: 0.9729 - recall_m: 0.9698 - val_loss: 2.3484 - val_acc: 0.5363 - val_f1_m: 0.5359 - val_precision_m: 0.5375 - val_recall_m: 0.5343\n",
      "Epoch 20/25\n",
      " - 1s - loss: 0.0830 - acc: 0.9689 - f1_m: 0.9689 - precision_m: 0.9712 - recall_m: 0.9666 - val_loss: 2.8664 - val_acc: 0.5363 - val_f1_m: 0.5363 - val_precision_m: 0.5370 - val_recall_m: 0.5357\n",
      "Epoch 21/25\n",
      " - 1s - loss: 0.0777 - acc: 0.9697 - f1_m: 0.9696 - precision_m: 0.9714 - recall_m: 0.9678 - val_loss: 2.8572 - val_acc: 0.5443 - val_f1_m: 0.5438 - val_precision_m: 0.5440 - val_recall_m: 0.5436\n",
      "Epoch 22/25\n",
      " - 1s - loss: 0.0698 - acc: 0.9724 - f1_m: 0.9728 - precision_m: 0.9742 - recall_m: 0.9714 - val_loss: 2.7572 - val_acc: 0.5337 - val_f1_m: 0.5348 - val_precision_m: 0.5373 - val_recall_m: 0.5324\n",
      "Epoch 23/25\n",
      " - 1s - loss: 0.0734 - acc: 0.9711 - f1_m: 0.9708 - precision_m: 0.9722 - recall_m: 0.9694 - val_loss: 2.6163 - val_acc: 0.5304 - val_f1_m: 0.5311 - val_precision_m: 0.5324 - val_recall_m: 0.5297\n",
      "Epoch 24/25\n",
      " - 1s - loss: 0.0818 - acc: 0.9686 - f1_m: 0.9685 - precision_m: 0.9704 - recall_m: 0.9667 - val_loss: 2.7348 - val_acc: 0.5244 - val_f1_m: 0.5237 - val_precision_m: 0.5250 - val_recall_m: 0.5225\n",
      "Epoch 25/25\n",
      " - 1s - loss: 0.0553 - acc: 0.9784 - f1_m: 0.9787 - precision_m: 0.9801 - recall_m: 0.9773 - val_loss: 2.8664 - val_acc: 0.5390 - val_f1_m: 0.5379 - val_precision_m: 0.5396 - val_recall_m: 0.5363\n",
      "1869/1869 [==============================] - 0s 121us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.2737795184040017,\n",
       " 0.5013376137131098,\n",
       " 0.5010328342278033,\n",
       " 0.5023818146168325,\n",
       " 0.49973247727651277]"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(vocab_size+1,\n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                         \n",
    "                            weights=[embedding_weights])\n",
    "\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)\n",
    "# Embedding\n",
    "x = embedding_layer(inputs)\n",
    "# Conv\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
    "x = Flatten()(x)  # (None, 8704)\n",
    "# Fully connected layers\n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "# Build model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy',f1_m,precision_m,recall_m])  # Adam, categorical_crossentropy\n",
    "model.summary()\n",
    "# model.fit(X_train, y_binary,batch_size=32,epochs=20,verbose=2)\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          batch_size=128,\n",
    "          epochs=25,\n",
    "          verbose=2)\n",
    "\n",
    "model.evaluate(X_test,y_test)\n",
    "# validation_data=(X_val, Y_val),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "u69y4QTZPPuo",
    "outputId": "11b72d93-8407-49bb-a4ba-785fb949957c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869/1869 [==============================] - 0s 104us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.2737795184040017,\n",
       " 0.5013376137131098,\n",
       " 0.5010328342278033,\n",
       " 0.5023818146168325,\n",
       " 0.49973247727651277]"
      ]
     },
     "execution_count": 127,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jja6sECUQFbS"
   },
   "outputs": [],
   "source": [
    "t=model.predict(X_test)\n",
    "t1=[]\n",
    "for i in t:\n",
    "  n2=[]\n",
    "  m=max(i)\n",
    "  for j in i:\n",
    "    if(j==m):\n",
    "      n2.append(1)\n",
    "    else:\n",
    "      n2.append(0)\n",
    "  t1.append(n2)  \n",
    "t1=np.array(t1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "eFn7WfDXQT4K",
    "outputId": "491c5c2d-e6fc-41d4-e535-d289bbd7bb0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7318361955085865, 0.5488771466314399, 0.7404227212681638],\n",
       " [0.3840909090909091, 0.6384892086330936, 0.4806949806949807],\n",
       " [0.555921052631579, 0.4241338112305854, 0.6675603217158177],\n",
       " [0.45430107526881724, 0.5096913137114142, 0.5589225589225589],\n",
       " 0.6737120211360634,\n",
       " 0.5010916994729945,\n",
       " 0.5492050618593274,\n",
       " 0.5076383159675968)"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This tells about the metrics for each individual class\n",
    "\n",
    "from statistics import mean\n",
    "acc=[0,0,0]\n",
    "pre=[0,0,0]\n",
    "f1=[0,0,0]\n",
    "rec=[0,0,0]\n",
    "for i in range(3):\n",
    "  acc[i]=accuracy_score(t1[:,i],y_test.todense()[:,i])\n",
    "  pre[i]=precision_score(t1[:,i],y_test.todense()[:,i])\n",
    "  rec[i]=recall_score(t1[:,i],y_test.todense()[:,i])\n",
    "  f1[i]=f1_score(t1[:,i],y_test.todense()[:,i])\n",
    "\n",
    "acc,pre,rec,f1,mean(acc),mean(pre),mean(rec),mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJBwzKIIXzA1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nlp3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
